
I am convinced that the only way to model planet temperature is with a single grid of multiple layers where all materials are present (ocean, air, land). This is what I intuit, but I need to either discover why I intuit this, or explain why my intuition is wrong. I will now attempt to do this.

In the past I've taken to different thinking. I thought about different approaches, but they all had one thing in common: they all partitioned the problem into isolated subcomponents (ocean, air, land). I thought that breaking the problem down this way would allow each subcomponent to be considered in isolation, and so I could tackle them one by one without having to consider their interactions. 

However this is not useful. At some point, you have to stitch the subcomponents together, and when this happens you will absolutely have to consider their interactions. And by this point, you will have already implemented the subcomponents in whichever manner was easiest. They will each have their own quirks and standards and assumptions, and these will have made life easier when implementing them, but it also means that whatever glue code you construct will have to work with whatever flimsy interfaces they can afford. Interfaces will have to be shoe horned in after the fact. The high level system design will be byzantine, and highly coupled to the implementation details of the subcomponents. If the design of a subcomponent must be reconsidered, then the entire high level system design will have to be reconsidered as well.

All throughout this I'm reflecting on a design principle I've heard used recently: whereever there is a system that is defined by subcomponents (whether it be in software, engineering, or management), the boundaries between subcomponents will determine where the problems occur in that system. Up to the present we've accomplished this using an organic bottom up approach. At each step we've built the most obvious low level functions and data structures. Functions and data structures are designed to be 100% uncoupled. Their methods accept only primitive data types, or at the very least, they accept data types which we officially endorse for use in passing between functions (e.g. rasters). This way, the only boundaries that define problems are the ones that emerge naturally, and there is absolutely zero coupling between data structures. It's actually worked out pretty well. 

It seems to get at a style of programming where we distinguish entities that are sanctioned for passing between functions (e.g. floats, vec3s, rasters, etc) from data structures that deal exclusively with passing and returning these entities. As we see with Grid objects, sanctioned entities are not necessarily primitive. They can be object oriented. They can inherit from one another, and can contain many encapsulated members. And as we see with our mass pool data structures, we can intend mass pools to be sanctioned for passing to other functions that operate at a higher level of abstraction, even though we forbid them to be passed amongst themselves. This is for the same reason we don't allow storing references within a class: if you want the banana, you shouldn't have to get the jungle. So the entities we saction for passing will vary depending on where we are in code. It's also interesting to note this exposes a formerly unarticulated deficiency of classic object oriented programming: it encourages an unhealthy conflation between passing-sanctioned entities and the namespaces that use them. 

The bottom up approach has served us well for a long time. But it only seems to have worked up to present because every time we hit a wall we were able to establish standards that allowed us to continue forward. There apparently needs to be some sort of top down development as well.

It seems in every case where we've hit a wall the standards we established to resolve our problems have worked by describing how low level functions and data structures behave. For example, we were only able to resolve early problems with spatial transport once we standardized on storing spatial information using structures of arrays, where arrays represented rasters. Before that point, we stored spatial information as arrays of structures, and we had no concept of a raster as an underlying algabraic object that could be passed between functions. We had no way to reuse code for conceptual operations that were happening frequently across disparate data structures, such as operations in vector calculus or binary morphology. In another example, we were only able to resolve problems with submodel design once we standardized on the entity component system, where components were composed of member attributes that are orthogonal, intrinsic, and extensive. Before that point, our submodels were designed with little rhyme or reason. They were tightly coupled, their attributes were disorganized, they could not trivially transfer mass between them, they required much more time to become familiar with, and worst of all, the behavior of their encapsulated state could not be trivially managed. 

And now again we are hitting a wall. We have subcomponent data structures to describe our mass pools such as ocean, air, and land. These include descriptions of their material properties and spatial distributions. However now we must tie them all together to determine how light propagates, enters our climate submodel as absorbed heat, and exits as emitted radiation. Whatever method we choose should also probably lend itself well to the climate submodel that we plan to implement, and it should incorporate the aspects that made all our earlier redesigns so successful. Altogether now, we have the following aspirations:

* create a single common data structure to be sanctioned for passing amongst subcomponent methods for light propagation, heat absorption, thermal emission, and climate modeling. 
* create an algebra surrounding the data structure (that is, a series of coherent operations that can be used to completely describe model behavior without being designed around it)
* use only orthogonal, intrinsic, and extensive members within the data structure
* make no use of cross referencing amongst data structures within the same level of the abstraction hierarchy

The requirement for orthogonal, intrinsic, and extensive data structure members is probably most strict, so we will start with that. It is fortunate that following this principle will make it trivial to construct an abelian group algebra (that is, an algebra with closure, commutativity, associativity, invertability, and identity). We just need to choose our data structure members. 

If we assume the interaction between subcomponents (e.g. ocean displacement) is handled elsewhere by other functions, then we can also assume that the subcomponents do not overlap with each other spatially. In this case, certain properties are additive which may not be otherwise:

* number density at a point in space
* column density between two points in space
* absorption of light between two points in space

Such properties could be trivially added together to produce a combined function with a similar method signature. This method signature would of course describe a given extensive property as a function of position. The climate model would then be developed around the outputs of this collection of functions. The steady state variant of our climate model might only consist of a single function that calls out to the aborption function mentioned above. As for the dynamic variant of our climate model, we should probably store heat at a given location as a state variable, since heat is an extensive property that cannot be described in a closed form expression across time. Since we want to leverage the fact that heat is extensive, we should allow output from the absorption function to be trivially added to the state variable that represents heat. Therefore, the state variable should be of the same data type as the output of the combined absorption function. This necessitates that the state variable for heat must store heat values for all subcomponents, indexed by location. Heat would be represented as a single raster on a spheroid grid with multiple vertically stacked layers. 

Other state variables within the climate model would also likely use a similar data type, for instance, pressure, precipitation or fluid velocity. Fluid velocity would likely be used jointly for both ocean currents and wind velocity. It is likely that other rasters would be used to indicate whether matter exists as a solid, liquid, or gas, so that 

Hopefully you will start to see how useful this could be. It is a very simple framework that minimizes the interfaces amongst subcomponents while maximizing the freedom we can excercise when using it. It would resolve design concerns for the upper levels of the abstraction hierarchy and give us a straightforward path to designing the remaining functionality of subcomponents. All thought could be focused on the design of the combined functions, and subcomponents would only be designed after the fact to cater to the specifications of these combined functions. If this requires the subcomponents to implement hacks, so be it - the hacks will be isolated to only a small portion of the system, and that small portion will be encapsulated well within the subcomponent. To do otherwise would require the high level design to implement hacks in order to cater to the implementation details of the subcomponents. We have also already seen how much the design of subcomponents can change wildly throughout the development of the model. If the high level design is to cater to the implementation details of subcomponents, the high level design will have to be constantly changed. This is exactly what we've seen in the model up to present, where low level utility functions tend to be the most beautiful parts of the model, and high level architecture tends to be whatever flavor-of-the-month architecture best supports the needs of low level functionality at the time. 





